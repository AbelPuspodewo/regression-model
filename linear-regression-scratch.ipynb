'''Import libraries'''
import numpy as np
import pandas as pd
from sklearn import metrics
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error

'''Google drive mounting and read it as pandas dataframe'''
from google.colab import drive
drive.mount('/content/gdrive')
data = pd.read_csv('gdrive/My Drive/Stock Predicting/airfoil_noise_data.csv')

'''Separate X(features) and Y(label), and convert into array'''
X = data.iloc[:, :-1].values
y = data.iloc[:, -1].values.reshape(-1,1)

'''Splitting dataset into train and test (80% and 20%)'''
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.2, random_state=41)

'''Build the model'''
class LinearRegression():
    def __init__(self, epochs, alpha):
        ''' constructor '''
        #Input hyperparameters
        self.epochs = epochs
        self.alpha = alpha
        #Initial values for w and b
        self.w = None
        self.b = 0
        
    def forward_propagation(self,X,w,b):
      #X has been transposed -> n x m
      #w -> 1 x n
      #b and z -> 1 x n
      z = (np.dot(w,X))+b
      return z

    def cost_function(self,z,y):
      m = y.shape[1]
      J = (1/(2*m))*np.sum(np.square(z-y))
      return J
    
    def back_propagation(self,X,y,z):
      m = y.shape[0]
      dz = (1/m)*(z-y)
      dw = np.dot(dz,X.T)
      db = np.sum(dz)
      return dw,db
    
    def gradient_descent(self,w,b,dw,db,alpha):
      w = w - self.alpha*dw
      b = b - self.alpha*db
      return w,b
    
    def fit(self, X_train, y_train, X_val, y_val, alpha, epochs):
        ''' function to train the tree '''
        #Store the ....
        m_train = y_train.shape[1]
        m_val = y_val.shape[1]

        #Store the number of features
        num_features = X_train.shape[0]
        print(num_features)

        #Initialize parameters w and b
        self.w = np.random.randn(1,num_features)
        self.b = 0

        #Transpose y_train
        Y_train = y_train.T

        for i in range(1,self.epochs+1):
          z_train = self.forward_propagation(X_train,self.w,self.b)
          cost_train = self.cost_function(z_train,Y_train)
          print(z_train.shape)
          print(Y_train.shape)
          dw,db = self.back_propagation(X_train,Y_train,z_train)
          self.w,self.b = self.gradient_descent(self.w,self.b,dw,db,alpha)
          MAE_train = (1/m_train)*np.sum(np.abs(z_train-Y_train))
          #z_val = self.forward_propagation(X_val,w,b)
          #cost_val = self.cost_function(z_val,y_val)
          #MAE_val = (1/m_val)*np.sum(np.abs(z_val.T-y_val))
          #print(MAE_train)
          #print(MAE_val)

          #print epochs dkk
          print('Epochs '+str(i)+'/'+str(epochs)+': ')
          print('Training Cost '+str(cost_train))
          print('Training MAE '+str(MAE_train))
    
    
    def predict(self, X_test, y_test):
        m_test = y_test.shape[1]
        Y_test = y_test.T
        z_test = self.forward_propagation(X_test,self.w,self.b)
        print(z_test.shape)
        print(y_test.shape)
        cost_test = self.cost_function(z_test,Y_test)
        MAE_test = (1/m_test)*np.sum(np.abs(z_test-Y_test))
        print(z_test)
        print(cost_test)
        print(MAE_test)

'''Define hyperparameters'''
alpha = 10e-12
epochs = 100

'''Train the model'''
regressor = LinearRegression(epochs,alpha)
regressor.fit(X_train, y_train, X_test, y_test, alpha, epochs)

'''Test the model'''
z = regressor.predict(X_test,y_test)
